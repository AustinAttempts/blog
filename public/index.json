[{"content":"AI Hype üöÄ Over the past year AI hype, specifically LLMs, has gotten out of hand. With the cultral populatiry of OpenAI and the general leap in sucess the space has seen in the past couple of years this hype isn\u0026rsquo;t unfounded. People who are not even technology adjacent are now quickly realizng the potential this advancment has to displace jobs on a scale not seen since the industrial revolution. This is causing a rush by practically every industry to be at the forefront of these developments.\nI, on the other hand, have been extremly wary of these tools. I tried my best to avoid them unil Nordic SemiConductor added an AI assistant to their Dev Zone. These microcontroller companies have a ton of resources however they are spread out through forums, app notes, white papers, and data sheets. This makes it a pain to locate and process without an FAEs help. This was the first time I used AI and felt it was genuinly useful. The tool would find a relevant response and do its best. It was often wrong, however, it was close enough that somone familiar with the technology could peace together a solution.\nWhen DeepSeek released their open source model that was comparable to the other major players I wanted to experiment which led me here\u0026hellip;\nOllama The open source community is amazing! Ollama is a tool specifically designed to host large language models (LLM) and they make it super easy. I have an old computer that I run as my lab computer and am intending to use to host from. Because of that I will be showing the Windows instructions I followed.\nInstallation Ollama has an easily accessible Windows download here. Once the installation is complete run the following command to verify it worked properly. 1 ollama version Downloading a Model Running the following command you will notice there are currently no models for use: 1 ollama list To add a model you must pull them. This can be achieved with the pull command. To get DeepSeek r1 this is the command. If you want a different model you can see the full list of available models here.\n1 ollama pull deepseek-r1 This installation can take a while but once complete you can re-run the list command to verify the model is accessible\nRun it Thats it! That is all the steps required you can simply run the model with the following command and it will open a shell in which you can type prompts and view responses. 1 ollama run deepseek-r1 Open-WebUI Now that the model is running lets make it easier to use and better looking than a CLI. Open WebUI is a tool for hosting Ollama and other models through their APIs. This will allow us to access our DeepSeek instance anywhere on my local network in a more presentable web insterface.\nTo begin there are a few options for installation and setup. I chose the python steps but their documentation suggests using the Docker Image. I should note it is important to update your Python3and pip installation to the latest version before continuing.\nCreate a virtual environment 1 python -m venv ~/open-webui-venv Activate the virtual environment 1 ~/open-webui-venv/Scripts/Activate.Ps1 Install Open WebUI 1 pip install open-webui Start the Server 1 open-webui serve All of these steps will show error logs in the case of a failure. If these ever occur they can be easily remediated by just reading the error and doing what is suggests.\nNow that the server is up and running you can access the applicaition from http://localhost:8080/\nConclusion This is not running on a powerful computer so the responses are slow but it is perfect for asking a question and letting it work it out while I do somthing else. For example, this blog post was edited by my own locally hosted AI! üòÉ\n","permalink":"http://localhost:1313/posts/self-hosting-ai/","summary":"\u003ch2 id=\"ai-hype-\"\u003eAI Hype üöÄ\u003c/h2\u003e\n\u003cp\u003eOver the past year AI hype, specifically LLMs, has gotten out of hand.  With the cultral populatiry of OpenAI and the general leap in sucess the space has seen in the past couple of years this hype isn\u0026rsquo;t unfounded.  People who are not even technology adjacent are now quickly realizng the potential this advancment has to displace jobs on a scale not seen since the industrial revolution.  This is causing a rush by practically every industry to be at the forefront of these developments.\u003cbr\u003e\nI, on the other hand, have been extremly wary of these tools.  I tried my best to avoid them unil Nordic SemiConductor added an AI assistant to their \u003ca href=\"https://devzone.nordicsemi.com/\"\u003eDev Zone\u003c/a\u003e.  These microcontroller companies have a ton of resources however they are spread out through forums, app notes, white papers, and data sheets.  This makes it a pain to locate and process without an FAEs help.  This was the first time I used AI and felt it was genuinly useful.  The tool would find a relevant response and do its best.  It was often wrong, however, it was close enough that somone familiar with the technology could peace together a solution.\u003c/p\u003e","title":"Austin Attempts: Self Hosting AI"},{"content":"Coming Soon\u0026hellip; ","permalink":"http://localhost:1313/posts/making-a-ring/","summary":"\u003ch2 id=\"coming-soon\"\u003eComing Soon\u0026hellip;\u003c/h2\u003e","title":"Austin Attempts: Making a Wedding Ring"},{"content":"Why did I start this? Honestly, I blame this entire journey on two people: Beau and Chris. As 2024 came to an end, I decided to focus on improving my coding skills. For the first time ever, I participated in Advent of Code and to make it even more challenging, I completed it entirely in a language that was completely new to me‚ÄîZig.1 This sparked a series of tangents, including learning RayLib, attempting to create a browser-based game, and eventually diving into Go to build an HTTP server.\nWhile going down this rabbit hole of learning, I finally decided to download Letterboxd to start recording my opinions on every movie I watched. Our household had already taken a liking to Goodreads, and as a proud AMC A-List member, I thought it would be fun to carry that habit over to my movie-watching in the new year. That‚Äôs when Beau, my roommate, suggested that since I enjoyed sharing my opinions so much, I should just start a blog. When I mentioned this idea to my friend Chris, he pointed out that it would be a perfect project to learn a bit of web development. And so, with all of that in mind\u0026hellip; here I am ü§∑‚Äç‚ôÇÔ∏è.\nThe idea for the blog was pretty straightforward:\nI didn‚Äôt want to write a bunch of custom HTML and CSS (it just isn‚Äôt enjoyable for me). There should be a simple way for visitors to search for specific posts. Ideally, I could write my posts in Markdown. Getting Started\u0026hellip; Kinda The more personal projects I‚Äôve attempted, the more I‚Äôve come to appreciate the truth of the saying, \u0026lsquo;You don‚Äôt know what you don‚Äôt know.\u0026rsquo; In both school and as a junior engineer, I took for granted how guided my efforts were at the start of a project. In those cases, someone with experience would outline the necessary steps to complete the work. Without those mentors, however, it can be daunting to figure out where to even begin.\nEvery project I‚Äôve attempted so far could be solved in countless ways, and to avoid being overwhelmed by the choices, I‚Äôve made it a habit to start with whatever tools are immediately available to me. While this often leads to less efficient or outright wrong solutions, it‚Äôs a valuable learning experience. Each misstep teaches me why the decision was suboptimal, preparing me for when it‚Äôs time to pivot to a better approach.\nAs I mentioned earlier, I had just started learning Go when this project idea came to mind, so my first attempt was, naturally, written in\u0026hellip; Go. Using the writing web applications guide from the Go documentation and a youtube series focused on building a blog, I was able to get a first version up and running surprisingly quickly.\nServeMux First, we create a ServeMux using Go‚Äôs standard http library. A ServeMux is an HTTP multiplexer that routes incoming requests to the appropriate handler based on the URL. It acts as a router, matching each request\u0026rsquo;s URL to its corresponding handler function.\n1 mux := http.NewServeMux() // allocates and returns a new [ServeMux]. File Reader Next, we need to process all the Markdown files containing my posts into a format that is usable. This is accomplished using GoldMark and FrontMatter to parse each file in a specified directory. At the time, I didn‚Äôt realize how helpful it would be to understand the inner workings of GoldMark and what it was doing behind the scenes.\n1 2 3 postReader := goBlog.FileReader{ Dir: \u0026#34;posts\u0026#34;, // local path in which to find files to parse } HTML templates Now that we have our content parsed and organized, we can move it to a template page. This template handles the bulk of the HTML and CSS needed to make the page look presentable. It defines the layout and styling for elements like the post\u0026rsquo;s title, author, and content, determining how and where this information is displayed on the webpage.\n1 postTemplate := template.Must(template.ParseFiles(\u0026#34;post.gohtml\u0026#34;)) Associate URL\u0026rsquo;s We add each post\u0026rsquo;s data to the previously created template and associate a handler to process the GET request for that specific URL.\n1 mux.HandleFunc(\u0026#34;GET /posts/{slug}\u0026#34;, goBlog.PostHandler(postReader, postTemplate)) Index all posts With all posts made and given a URL we now need a landing page. This landing page has its own template file and can display whatever I want.\n1 2 indexTemplate := template.Must(template.ParseFiles(\u0026#34;index.gohtml\u0026#34;)) mux.HandleFunc(\u0026#34;GET /\u0026#34;, goBlog.IndexHandler(postReader, indexTemplate)) Run Site With all the functionality in place, we can now run the server locally on port 8080.\n1 log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, mux)) Great, it‚Äôs working! Now what? Well, aside from the lack of features, there‚Äôs one major issue: I need to host a server somewhere that can handle all the functionality mentioned above. This setup works perfectly for development and testing on my home setup, but how can I make the site accessible to the public?\nHUGO This blog is still a work in progress. I am still adding things so this post needs to grow with it\nAdvent of Code was an extremely fun experience, and I‚Äôll likely dedicate a series of posts to these puzzles.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/writing-a-blog/","summary":"\u003ch2 id=\"why-did-i-start-this\"\u003eWhy did I start this?\u003c/h2\u003e\n\u003cp\u003eHonestly, I blame this entire journey on two people: Beau and Chris. As 2024 came to an end, I decided to focus on improving my coding skills. For the first time ever, I participated in \u003ca href=\"https://adventofcode.com/\"\u003eAdvent of Code\u003c/a\u003e and to make it even more challenging, I completed it entirely in a language that was completely new to me‚ÄîZig.\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e This sparked a series of tangents, including learning RayLib, attempting to create a browser-based game, and eventually diving into Go to build an HTTP server.\u003c/p\u003e","title":"Austin Attempts: Writing a Blog"}]